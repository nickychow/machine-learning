{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Programming Exercise 1 - Rust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "In this exercise, you will implement linear regression and get to see it work on data. Before starting on this programming exercise, we strongly recommend watching the video lectures and completing the review questions for the associated topics.\n",
    "\n",
    "To get started with the exercise, you will need to download the starter code and unzip its contents to the directory where you wish to complete the exercise. If needed, use the cd command in Octave/MATLAB to change to this directory before starting this exercise.\n",
    "\n",
    "Prepare the rust environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    ":dep polars = { version = \"0.20.0\", features = [\"lazy\", \"csv-file\", \"ndarray\"] }\n",
    ":dep plotly = { version = \"0.7.0\", features = [\"plotly_ndarray\"] }\n",
    ":dep ndarray = \"0.15.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "failed to resolve: use of undeclared crate or module `polars`",
     "output_type": "error",
     "traceback": [
      "use of undeclared crate or module `polars`",
      "failed to resolve: use of undeclared crate or module `polars`"
     ]
    }
   ],
   "source": [
    "use polars::prelude::*;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable\n",
    "\n",
    "In this part of this exercise, you will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities. You would like to use this data to help you select which city to expand to next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "failed to resolve: use of undeclared type `Schema`",
     "output_type": "error",
     "traceback": [
      "let mut schema = Schema::new();",
      "                 ^^^^^^ use of undeclared type `Schema`",
      "failed to resolve: use of undeclared type `Schema`"
     ]
    },
    {
     "ename": "Error",
     "evalue": "failed to resolve: use of undeclared type `DataType`",
     "output_type": "error",
     "traceback": [
      "schema.with_column(\"population\".to_string(), DataType::Float64);",
      "                                             ^^^^^^^^ use of undeclared type `DataType`",
      "failed to resolve: use of undeclared type `DataType`"
     ]
    },
    {
     "ename": "Error",
     "evalue": "failed to resolve: use of undeclared type `DataType`",
     "output_type": "error",
     "traceback": [
      "schema.with_column(\"profit\".to_string(), DataType::Float64);",
      "                                         ^^^^^^^^ use of undeclared type `DataType`",
      "failed to resolve: use of undeclared type `DataType`"
     ]
    },
    {
     "ename": "Error",
     "evalue": "failed to resolve: use of undeclared type `LazyCsvReader`",
     "output_type": "error",
     "traceback": [
      "let mut df = LazyCsvReader::new(\"./ex1data1.txt\".into()).with_schema(Arc::new(dbg!(schema))).has_header(false).finish()?.collect()?;",
      "             ^^^^^^^^^^^^^ use of undeclared type `LazyCsvReader`",
      "failed to resolve: use of undeclared type `LazyCsvReader`"
     ]
    }
   ],
   "source": [
    "// rust load data from ex1data1.txt\n",
    "use std::sync::Arc;\n",
    "\n",
    "let mut schema = Schema::new();\n",
    "schema.with_column(\"population\".to_string(), DataType::Float64);\n",
    "schema.with_column(\"profit\".to_string(), DataType::Float64);\n",
    "\n",
    "let mut df = LazyCsvReader::new(\"./ex1data1.txt\".into()).with_schema(Arc::new(dbg!(schema))).has_header(false).finish()?.collect()?;\n",
    "\n",
    "// dbg!(&df);\n",
    "dbg!(df.head(Some(3)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data\n",
    "\n",
    "Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (profit and population). Many other problems that you will encounter in real life are multi-dimensional and can't be plotted on a 2-d plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "markers",
         "type": "scatter",
         "x": [
          6.1101,
          5.5277,
          8.5186,
          7.0032,
          5.8598,
          8.3829,
          7.4764,
          8.5781,
          6.4862,
          5.0546,
          5.7107,
          14.164,
          5.734,
          8.4084,
          5.6407,
          5.3794,
          6.3654,
          5.1301,
          6.4296,
          7.0708,
          6.1891,
          20.27,
          5.4901,
          6.3261,
          5.5649,
          18.945,
          12.828,
          10.957,
          13.176,
          22.203,
          5.2524,
          6.5894,
          9.2482,
          5.8918,
          8.2111,
          7.9334,
          8.0959,
          5.6063,
          12.836,
          6.3534,
          5.4069,
          6.8825,
          11.708,
          5.7737,
          7.8247,
          7.0931,
          5.0702,
          5.8014,
          11.7,
          5.5416,
          7.5402,
          5.3077,
          7.4239,
          7.6031,
          6.3328,
          6.3589,
          6.2742,
          5.6397,
          9.3102,
          9.4536,
          8.8254,
          5.1793,
          21.279,
          14.908,
          18.959,
          7.2182,
          8.2951,
          10.236,
          5.4994,
          20.341,
          10.136,
          7.3345,
          6.0062,
          7.2259,
          5.0269,
          6.5479,
          7.5386,
          5.0365,
          10.274,
          5.1077,
          5.7292,
          5.1884,
          6.3557,
          9.7687,
          6.5159,
          8.5172,
          9.1802,
          6.002,
          5.5204,
          5.0594,
          5.7077,
          7.6366,
          5.8707,
          5.3054,
          8.2934,
          13.394,
          5.4369
         ],
         "y": [
          17.592,
          9.1302,
          13.662,
          11.854,
          6.8233,
          11.886,
          4.3483,
          12,
          6.5987,
          3.8166,
          3.2522,
          15.505,
          3.1551,
          7.2258,
          0.71618,
          3.5129,
          5.3048,
          0.56077,
          3.6518,
          5.3893,
          3.1386,
          21.767,
          4.263,
          5.1875,
          3.0825,
          22.638,
          13.501,
          7.0467,
          14.692,
          24.147,
          -1.22,
          5.9966,
          12.134,
          1.8495,
          6.5426,
          4.5623,
          4.1164,
          3.3928,
          10.117,
          5.4974,
          0.55657,
          3.9115,
          5.3854,
          2.4406,
          6.7318,
          1.0463,
          5.1337,
          1.844,
          8.0043,
          1.0179,
          6.7504,
          1.8396,
          4.2885,
          4.9981,
          1.4233,
          -1.4211,
          2.4756,
          4.6042,
          3.9624,
          5.4141,
          5.1694,
          -0.74279,
          17.929,
          12.054,
          17.054,
          4.8852,
          5.7442,
          7.7754,
          1.0173,
          20.992,
          6.6799,
          4.0259,
          1.2784,
          3.3411,
          -2.6807,
          0.29678,
          3.8845,
          5.7014,
          6.7526,
          2.0576,
          0.47953,
          0.20421,
          0.67861,
          7.5435,
          5.3436,
          4.2415,
          6.7981,
          0.92695,
          0.152,
          2.8214,
          1.8451,
          4.2959,
          7.2029,
          1.9869,
          0.14454,
          9.0551,
          0.61705
         ]
        }
       ],
       "layout": {}
      }
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// rust code to plot the data\n",
    "\n",
    "use plotly::{Plot,Scatter};\n",
    "use plotly::common::{Mode, Title};\n",
    "use plotly::ndarray::ArrayTraces;\n",
    "use ndarray::prelude::*;\n",
    "\n",
    "let mut plot = Plot::new();\n",
    "let ndarray = df.to_ndarray::<Float64Type>()?;\n",
    "let trace = Scatter::from_array(ndarray.slice(s![..,0]).into_owned(),ndarray.slice(s![..,1]).into_owned()).mode(Mode::Markers);\n",
    "plot.add_trace(trace);\n",
    "// let layout = Layout::new().height(800);\n",
    "// plot.set_layout(layout);\n",
    "plot.lab_display();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "In this section, you will fit the linear regression parameters to our dataset using gradient descent.\n",
    "\n",
    "#### Update Equations\n",
    "\n",
    "The objective of linear regression is to minimize the cost function\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m}\\sum\\limits_{i=1}^m ( h_{\\theta}(x^{(i)})-y^{(i)} )^{2}\n",
    "$$\n",
    "\n",
    "where the hypothesis $h_\\theta(x)$ is given by the linear model\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta^Tx =\\theta_{0}+\\theta_{1}x_1\n",
    "$$\n",
    "\n",
    "The training examples are stored in X row-wise, like such:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "  x^{(1)}_0 & x^{(1)}_1  \\newline\n",
    "  x^{(2)}_0 & x^{(2)}_1  \\newline\n",
    "  x^{(3)}_0 & x^{(3)}_1\n",
    "\\end{bmatrix}\n",
    "&,\\theta =\n",
    "\\begin{bmatrix}\n",
    "  \\theta_0 \\newline\n",
    "  \\theta_1 \\newline\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "You can calculate the hypothesis as a column vector of size (m x 1) with:\n",
    "\n",
    "$$\n",
    "{{h}_{\\theta }}\\left( X \\right)=X{\\theta }\n",
    "$$\n",
    "\n",
    "Recall that the parameters of your model are the $\\theta$ values. These are the values you will adjust to minimize cost $J(\\theta)$. One way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}((h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\\ (\\text{simultaneously update } \\theta_j \\text{ for all }j) \n",
    "$$\n",
    "\n",
    "With each step of gradient descent, your parameters $j$ come closer to the optimal values that will achieve the lowest cost $j(\\theta)$.\n",
    "\n",
    "**Implementation Note:** We store each example as a row in the the X matrix in MATLAB. To take into account the intercept term ($\\theta_0$), we add an additional first column to **X** and set it to all ones. This allows us to treat $\\theta_0$ as simply another 'feature'.\n",
    "\n",
    "##### Vectorized Implementation\n",
    "\n",
    "Vectorizations is the act of replacing the loops in a computer program with matrix operations. If you have a good linear algebra library (like numpy), the library will optimize the code automatically for the computer the code runs on. Mathematically, the 'regular' function should mean the same as the vectorized function.\n",
    "\n",
    "Gradient descent vectorized: $\\theta = \\frac{1}{2m}(X\\theta - \\vec{y})^T(X\\theta-\\vec{y})$\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "In this script, we have already set up the data for linear regression. In the following lines, we add another dimension to our data to accommodate the $\\theta_0$ intercept term. Run the code below to initialize the parameters to 0 and the learning rate alpha to 0.01.\n",
    "\n",
    "#### Computing the cost $J(\\theta)$\n",
    "\n",
    "As you perform gradient descent to minimize the cost function $J(\\theta)$, it is helpful to monitor the convergence by computing the cost. In this section, you will implement a function to calculate $J(\\theta)$ so you can check the convergence of your gradient descent implementation.\n",
    "\n",
    "**Exercise:** Implement a vectorized implementation of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// rust code to compute the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "Next, you will implement gradient descent. The loop structure has been written for you, and you only need to supply the updates to  within each iteration.\n",
    "\n",
    "As you program, make sure you understand what you are trying to optimize and what is being updated. Keep in mind that the cost $J(\\theta)$ is parameterized by the vector $\\theta$, not $X$ and $y$. That is, we minimize the value of $J(\\theta)$ by changing the values of the vector $\\theta$, not by changing $X$ or $y$. Refer to the equations given earlier and to the video lectures if you are uncertain.\n",
    "\n",
    "A good way to verify that gradient descent is working correctly is to look at the value of $J$ and check that it is decreasing with each step. If it is not, you may need to increase the learning rate $\\alpha$.\n",
    "\n",
    "After you are finished, run this execute this section. The code below will use your final parameters to plot the linear fit. The result should look something like Figure 2 below:\n",
    "\n",
    "Your final values for  will also be used to make predictions on profits in areas of 35,000 and 70,000 people.\n",
    "\n",
    "We want are hypothesis $h_\\theta(x)$ to function as good as possibly. Therefore, we want to minimalize the cost function J(\\theta). Gradient descent is an algorithm used to do that.\n",
    "\n",
    "The formal definition of gradient descent:\n",
    "\n",
    "$$\n",
    "repeat \\ \\{ \\\\ \\enspace \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\displaystyle\\sum_{i = 1}^{m}(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\\\\\\}\n",
    "$$\n",
    "\n",
    "An illustration of gradient descent on a single variable:\n",
    "\n",
    "![gradientdescent](https://github.com/rickwierenga/CS229-Python/raw/8c899e94c7bdf60031ce0b80402285820ce1cb44/ex1/notes/gradientdescent.png)\n",
    "\n",
    "**Exercise:** Implement the gradient descent algorithm in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// rust code to compute the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with multiple variables - Multivariate Linear Regression\n",
    "\n",
    "In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
    "\n",
    "The file `ex1data2.txt` contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house. Run this section now to preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// rust code to load ex1data2.txt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization\n",
    "\n",
    "This section of the script will start by loading and displaying some values from this dataset. By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly.\n",
    "\n",
    "Your task here is to complete the code to:\n",
    "\n",
    "- Subtract the mean value of each feature from the dataset.\n",
    "- After subtracting the mean, additionally scale (divide) the feature values by their respective \"standard deviations\".\n",
    "\n",
    "The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within $\\pm 2$ standard deviations of the mean).\n",
    "\n",
    "You will do this for all the features and your code should work with datasets of all sizes (any number of features / examples). Note that each column of the matrix **X** corresponds to one feature.\n",
    "\n",
    "**Implementation Note:** When normalizing the features, it is important to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters from the model, we often want to predict the prices of houses we have not seen before. Given a new x value (living room area and number of bedrooms), we must first normalize x using the mean and standard deviation that we had previously computed from the training set.\n",
    "\n",
    "**Add the bias term**\n",
    "Now that we have normailzed the features, we again add a column of ones corresponding to  to the data matrix X.\n",
    "\n",
    "When features differ by order of magnitude, first performing feature scaling can make gradient descent converge much more quickly. Formally:\n",
    "\n",
    "$$\n",
    "x := \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n",
    "\n",
    "**Important:** It is crucial to store $\\mu$ and $\\sigma$ if you want to make predictions using the model later.\n",
    "\n",
    "**Exercise:** Perform feature normalization on the following dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// rust code to normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Previously, you implemented gradient descent on a univariate regression problem. The only difference now is that there is one more feature in the matrix X. The hypothesis function and the batch gradient descent update rule remain unchanged.\n",
    "\n",
    "You should complete the code to implement the cost function and gradient descent for linear regression with multiple variables. If your code in the previous part (single variable) already supports multiple variables, you can use it here too.\n",
    "\n",
    "Make sure your code supports any number of features and is well-vectorized. You can use the command size(X,2) to find out how many features are present in the dataset.\n",
    "\n",
    "We have provided you with the following starter code below that runs gradient descent with a particular learning rate (alpha). Your task is to first make sure that your functions computeCost and gradientDescent already work with this starter code and support multiple variables.\n",
    "\n",
    "**Implementation Note:** In the multivariate case, the cost function can also be written in the following vectorized form:\n",
    "\n",
    "$$\n",
    "J(\\theta)=\\frac{1}{2m}\\left(X\\theta-\\vec{y}\\right)^T\\left(X\\theta-\\vec{y}\\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "    -(x^{(1)})^T -) \\\\\n",
    "    -(x^{(2)})^T -) \\\\\n",
    "    \\vdots \\\\\n",
    "    -(x^{(m)})^T -) \\\\\n",
    "    \\end{bmatrix},\n",
    "\\ \\ \n",
    "\\vec{y} = \\begin{bmatrix}\n",
    "    y^{(1)} \\\\\n",
    "    y^{(2)} \\\\\n",
    "    \\vdots \\\\\n",
    "    y^{(m)} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The vectorized version is efficient when you're working with numerical computing tools like MATLAB. If you are an expert with matrix operations, you can prove to yourself that the two forms are equivalent.\n",
    "\n",
    "Finally, you should complete and run the code below to predict the price of a 1650 sq-ft, 3 br house using the value of theta obtained above. \n",
    "\n",
    "**Hint:** At prediction, make sure you do the same feature normalization. Recall that the first column of X is all ones. Thus, it does not need to be normalized.\n",
    "\n",
    "Remember the algorithm for gradient descent:\n",
    "\n",
    "$$\n",
    "repeat \\ \\{ \\\\ \\enspace \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\displaystyle\\sum_{i = 1}^{m}(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\\\\\\}\n",
    "$$\n",
    "\n",
    "The vectorization for multivariate gradient descent:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\frac{\\alpha}{m}X^T(X\\theta - \\vec{y})\n",
    "$$\n",
    "\n",
    "**Exercise:** Implement gradient descent for multiple features. Make sure your solution is vectorized and supports any number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// rust code to implement gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations\n",
    "\n",
    "In the lecture videos, you learned that the closed-form solution to linear regression is\n",
    "\n",
    "$$\n",
    "\\theta = \\left(X^T X\\right)^{-1} X^T \\vec{y} \n",
    "$$\n",
    "\n",
    "Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no \"loop until convergence\" like in gradient descent.\n",
    "\n",
    "Complete the code to use the formula above to calculate $\\theta$, then run the code in this section. Remember that while you don't need to scale your features, we still need to add a column of 1's to the X matrix to have an intercept term $(\\theta_0)$ . Note that the code below will add the column of 1's to X for you.\n",
    "\n",
    "**Optional (ungraded) exercise:** Now, once you have found  using this method, use it to make a price prediction for a 1650-square-foot house with 3 bedrooms. You should find that gives the same predicted price as the value you obtained using the model fit with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// rust code to implement normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- https://levelup.gitconnected.com/machine-learning-and-rust-part-3-smartcore-dataframe-and-linear-regression-10451fdc2e60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
